% !TEX root =  master.tex
\chapter{Grundlagen}
\section{Arc Raiders \& Gaming Tools}

\subsection{Companion Applications im digitalen Ökosystem}
Companion Applications haben sich als eigenständige Software-Kategorie etabliert, die primäre Anwendungen durch zusätzliche Funktionalität ergänzt, ohne das Hauptprodukt zu ersetzen. Im Gaming-Kontext erfüllen diese Anwendungen mehrere Schlüsselfunktionen: Sie erweitern die Spielerfahrung über die Session hinaus, bieten Planungs- und Analysewerkzeuge und ermöglichen Social Features außerhalb der Spielumgebung.

Companion Apps für Gaming-Konsolen verzeichneten seit 2020 ein signifikantes Wachstum, wobei die PC-Gaming-Plattform Steam mit etwa 33,4 Millionen Downloads im zuletzt gemessenen Quartal führend war~\cite{Statista2024ConsoleApps}. Diese Entwicklung zeigt, dass Spieler bereit sind, zusätzliche Tools zu nutzen, um ihre Gaming-Erfahrung zu optimieren.

Die technische Architektur von Companion Apps variiert je nach Anwendungsfall:
\begin{itemize}
    \item \textbf{Offizielle Plattform-Apps:} Direkte Integration mit Herstellersystemen (PlayStation App, Xbox App, Steam Mobile)
    \item \textbf{Game-spezifische Tools:} Fokus auf ein einzelnes Spiel oder Franchise
    \item \textbf{Community-getriebene Lösungen:} Von Spielern entwickelte Third-Party-Tools
\end{itemize}

\subsection{Gaming Companion Apps: Funktionale Kategorisierung}

Basierend auf der Analyse existierender Lösungen lassen sich Gaming Companion Apps in folgende funktionale Kategorien einteilen:

\textbf{Informations- und Datenbank-Tools:} Diese Tools bieten Zugriff auf Spieldaten wie Item-Statistiken, Charakterwerte oder Mechaniken. Apps wie Handbook for EFT bieten Spielern offline verfügbare Informationen zu Karten, Munitionsvergleichen, Waffen-Performance, Ausrüstung, Quest-Guides und Key-Informationen~\cite{HandbookEFT2024}.

\textbf{Progress-Tracking-Systeme:} Apps wie The Hideout: Tarkov Sidekick ermöglichen Quest-Tracking, Hideout-Modul-Verwaltung und Team-Quest-Tracking, wo Spieler den Status ihrer Freunde sehen können~\cite{TheHideoutApp2024}.

\textbf{Markt- und Wirtschafts-Tools:} Funktionen wie Flea Market-Preisanzeige, Preishistorien bis zu einem Jahr zurück und Preis-Alerts für Flea Market-Preise helfen Spielern bei wirtschaftlichen Entscheidungen~\cite{TheHideoutApp2024}.

\textbf{Karten- und Navigationshilfen:} Interactive Maps-Apps bieten detaillierte, community-erstellte Karten mit Loot-Spots, Extraktionspunkten, Key-Locations und Quest-Details~\cite{GameMapsOverwolf2024}.

\textbf{Build-Planer und Kalkulatoren:} Weapon Builder und Damage Calculator ("`Tarkov'd Simulator"') ermöglichen theoretisches Durchspielen von Szenarien vor der Implementierung im Spiel~\cite{TheHideoutApp2024}.

\subsection{Extraction Shooter: Genre-spezifische Anforderungen}

Extraction Shooter basieren auf Konzepten der Verlustaversion und verzögerten Gratifikation, wobei jeder Raid ein Risiko darstellt und der erfolgreiche Abschluss eines wertvollen Durchgangs intensive dopamingesteuerte Befriedigung freisetzt~\cite{InsiderGaming2025ExtractionShooters}. Diese psychologischen Mechanismen erzeugen spezifische Anforderungen an unterstützende Tools.

\textbf{Risiko-Management:} Die Permadeath-Mechanik von Extraction Shootern erfordert sorgfältige Planung. Companion Apps können helfen, Risiken zu minimieren durch:
\begin{itemize}
    \item Vorausschauende Ressourcenplanung
    \item Optimale Route-Planung zur Minimierung von Begegnungen
    \item Wert-Kalkulation von Loot vs. Risiko
\end{itemize}

\textbf{Komplexitätsreduktion:} Das Extraction-Shooter-Genre gilt aufgrund seiner Komplexität als schwer zugänglich für den Massenmarkt~\cite{IconEra2024ExtractionMarket}. Arc Raiders hat durch seine Betonung von Teamsynergien und intuitiveren Spielerführungssystemen einen zugänglicheren Einstiegspunkt geschaffen~\cite{InsiderGaming2025ExtractionShooters}.

Der aktuelle Markt konzentriert sich auf Escape from Tarkov (ca.~60.000 gleichzeitige Nutzer), Hunt Showdown (ca.~20.000) und Dark and Darker (ca.~10.000), zusammen etwa 100.000 gleichzeitige Nutzer~\cite{IconEra2024ExtractionMarket}. Die Herausforderung für neue Titel liegt in der Etablierung eigener Tool-Ökosysteme.

\subsection{Arc Raiders: Technische Charakteristika und Systeme}

ARC Raiders ist ein 2025 veröffentlichter Third-Person-Extraction-Shooter, entwickelt mit der Unreal Engine 5 für PlayStation 5, Windows und Xbox Series X/S~\cite{WikipediaARCRaiders2025}. Bis zum 11.~November 2025 hatte das Spiel weltweit über vier Millionen Exemplare verkauft~\cite{WikipediaARCRaiders2025}, was eine signifikante Nutzerbasis für Companion-Tools darstellt.

Das Spiel implementiert mehrere Systeme, die durch externe Tools unterstützt werden können:

\textbf{Quest-System:} Spieler erfüllen Quests für Händler mit unterschiedlichen Motiven und Agenden, was sich in komplexen Quest-Chains mit Abhängigkeiten manifestiert~\cite{SteamARCRaiders2025}. Das Ingame-Interface zeigt nur aktive Quests, was einen Gesamtüberblick erschwert.

\textbf{Crafting und Ressourcen:} Spieler müssen Workshop-Stationen upgraden und Blueprints lernen, um fortgeschrittenere Items zu craften~\cite{SteamARCRaiders2025}. Bei begrenztem Inventarplatz ist strategisches Ressourcen-Management essentiell.

\textbf{Skill-Progression:} Der ARC Raiders Skill-Tree verzweigt sich in drei Pfade: Survival, Mobility und Conditioning~\cite{SteamARCRaiders2025}, was Langzeitplanung erfordert.

\textbf{Multiplayer-Koordination:} Das Spiel unterstützt nahtloses Cross-Platform-Spiel zwischen PlayStation, Xbox und PC, wobei Spieler in Squads bis zu drei Personen oder solo spielen können~\cite{SteamARCRaiders2025}.

\subsection{Referenzimplementierungen: Tarkov Companion Ecosystem}

Das Escape from Tarkov Ökosystem bietet wertvolle Erkenntnisse für die Entwicklung von Companion Apps.

Tarkov Companion als Overwolf-App bietet Quest-Management sortiert nach Händler, Location oder Status, Browse-Funktionen für Karten mit Extraktionen, Loot-Hotspots und Quest-Items sowie Key-Suche und Hideout-Upgrade-Tracking~\cite{OverwolfTarkovCompanion2024}. Diese Features repräsentieren den aktuellen Standard für Extraction-Shooter-Companion-Apps.

Tarkov.dev bietet ein freies, community-erstelltes und Open-Source-Ökosystem von Escape from Tarkov-Tools inklusive Informationen zu Items, Crafts, Barters, Maps, Loot-Tiers, Hideout-Profiten und einer freien API~\cite{TarkovDev2024}. Die Verfügbarkeit einer API ermöglicht die Entwicklung vielfältiger Third-Party-Tools.


\section{Moderne Web-Technologien}

Die Wahl geeigneter Web-Technologien ist entscheidend für den langfristigen Erfolg komplexer Webanwendungen. Für die Entwicklung einer Arc Raiders Companion App kommen moderne, produktionsreife Technologien zum Einsatz, die sowohl Entwicklerproduktivität als auch Anwendungsperformance optimieren.

\subsection{TypeScript}

TypeScript hat sich als De-facto-Standard für moderne JavaScript-Entwicklung etabliert. Als Superset von JavaScript fügt TypeScript statische Typisierung und erweiterte Sprachfeatures hinzu, die besonders für große Projekte wertvoll sind~\cite{Invicta2024TypeScriptLargeScale}.

\textbf{Type Safety in großen Projekten:} Type Safety ist eines der herausragenden Merkmale von TypeScript und adressiert eine kritische Limitierung des dynamischen Typsystems von JavaScript~\cite{Invicta2024TypeScriptLargeScale}. In größeren Anwendungen, wo mehrere Entwickler an der selben Codebase arbeiten, können unterschiedliche Annahmen über Datentypen zu unerwartetem Verhalten und schwer auffindbaren Bugs führen~\cite{Invicta2024TypeScriptLargeScale}. TypeScript ermöglicht es Entwicklern, Typen explizit für Variablen, Funktionsparameter und Rückgabewerte zu definieren, wodurch potenzielle Fehler zur Compile-Zeit statt zur Laufzeit erkannt werden~\cite{Invicta2024TypeScriptLargeScale}.

Organisationen übernehmen TypeScript zunehmend für Large-Scale-Anwendungen aufgrund seiner Fähigkeit, Fehler zur Compile-Zeit zu erkennen, wodurch Laufzeitfehler reduziert und die Code-Qualität verbessert wird~\cite{Expedite2024TypeScript}. Der strukturierte Ansatz von TypeScript hilft Teams, komplexe Codebases effizienter zu verwalten~\cite{Expedite2024TypeScript}. In größeren Teams verbessert Type Safety die Zusammenarbeit durch Reduzierung der kognitiven Last und Förderung klarerer Kommunikation~\cite{Invicta2024TypeScriptLargeScale}.

\textbf{Developer Experience:} TypeScript verbessert nicht nur die Code-Qualität, sondern steigert auch signifikant die Developer Experience durch überlegenes Tooling und Error Reporting~\cite{Invicta2024TypeScriptLargeScale}. Die Integration mit modernen IDEs bietet erweiterte Features wie intelligente Code-Vervollständigung, automatisches Refactoring und Code-Navigation~\cite{Djirdeh2024TypeScriptBestPractices}. 

Tooling und IDE-Unterstützung für TypeScript erfuhren 2024 signifikante Verbesserungen, wobei Entwickler von besserem IntelliSense, Auto-Completion und Refactoring-Tools profitieren, was den Entwicklungsprozess reibungsloser und effizienter gestaltet~\cite{Toxigon2025TypeScriptTrends}. TypeScript wird zunehmend mit modernen Frameworks wie React, Angular und Vue.js integriert, was Entwicklern ermöglicht, die Vorteile von TypeScripts Type-Checking zu nutzen und gleichzeitig die leistungsstarken Features dieser Frameworks für den Aufbau von Benutzeroberflächen zu verwenden~\cite{Expedite2024TypeScript}.

Durch die Nutzung des statischen Typsystems von TypeScript können Entwickler sichereren und wartbareren Code schreiben und die Gesamtqualität und Zuverlässigkeit ihrer Anwendungen verbessern~\cite{Djirdeh2024TypeScriptBestPractices}. Mit dem Fokus auf statische Typisierung und Developer-Ergonomie ist TypeScript gut positioniert, um den sich entwickelnden Anforderungen der Webentwicklung auch zukünftig gerecht zu werden~\cite{Bhatu2024TypeScript}.

\subsection{React}

React hat sich als führende JavaScript-Bibliothek für den Aufbau von Benutzeroberflächen etabliert, primär für \ac{SPA}~\cite{GeeksForGeeks2025ReactComponent}. Eines der wichtigsten Features von React ist seine komponentenbasierte Architektur, die Entwicklern ermöglicht, skalierbare und wartbare Anwendungen effizient zu erstellen.

\textbf{Komponentenbasiertes \ac{UI}:} In React ist eine Komponente eine wiederverwendbare, eigenständige Einheit einer Benutzeroberfläche~\cite{GeeksForGeeks2025ReactComponent}. Komponenten ermöglichen es, eine Anwendung in kleinere, unabhängige Teile zu zerlegen, die effizient verwaltet und wiederverwendet werden können~\cite{GeeksForGeeks2025ReactComponent}. Diese modulare Struktur macht die Anwendung einfacher zu entwickeln, zu warten und zu skalieren, da Komponenten über verschiedene Teile der App oder sogar in unterschiedlichen Projekten wiederverwendet werden können~\cite{GeeksForGeeks2025ReactComponent}.

Jede React-Anwendung besteht aus einem Baum von Komponenten, wobei jede Komponente ihre eigene Logik, ihren State und ihre \ac{UI}-Repräsentation hat~\cite{GeeksForGeeks2025ReactComponent}. Die Vorteile dieser Architektur umfassen:
\begin{itemize}
    \item \textbf{Wiederverwendbarkeit:} Komponenten können mehrfach in verschiedenen Teilen einer Anwendung verwendet werden
    \item \textbf{Modularität:} Jede Komponente handhabt ein spezifisches Stück Funktionalität, was die Anwendung strukturierter macht
    \item \textbf{Skalierbarkeit:} Große Anwendungen können durch Zusammensetzen kleinerer, wiederverwendbarer Komponenten entwickelt werden
    \item \textbf{Wartbarkeit:} Updates und Bugfixes sind einfacher, da Änderungen auf spezifische Komponenten lokalisiert sind~\cite{GeeksForGeeks2025ReactComponent}
\end{itemize}

\textbf{Komposition over Inheritance:} React verfügt über ein leistungsstarkes Kompositionsmodell, und die Verwendung von Komposition anstelle von Vererbung wird empfohlen, um Code zwischen Komponenten wiederzuverwenden~\cite{ReactDocs2024Composition}. Bei Facebook verwenden die Entwickler React in tausenden von Komponenten, und es wurden keine Anwendungsfälle gefunden, bei denen die Erstellung von Komponenten-Vererbungshierarchien empfohlen würde~\cite{ReactDocs2024Composition}. Props und Komposition bieten die gesamte Flexibilität, die benötigt wird, um das Aussehen und Verhalten einer Komponente auf explizite und sichere Weise anzupassen~\cite{ReactDocs2024Composition}.

Komposition ist eine Technik, bei der eine Komponente durch Zusammensetzen anderer Komponenten aufgebaut wird, ähnlich wie man ein Lied aus verschiedenen musikalischen Noten komponieren würde~\cite{StudyRaid2024Composition}. React fördert die Verwendung von Komposition anstelle von Vererbung aus mehreren Gründen:

\begin{itemize}
    \item \textbf{Flexibilität:} Komposition gibt mehr Flexibilität bei der gemeinsamen Nutzung von Funktionalität zwischen Komponenten~\cite{StudyRaid2024Composition}
    \item \textbf{Einfachheit:} Sie fördert einfachere Hierarchien mit Komponenten, die isoliert verstanden werden können, ohne eine Vererbungskette kennen zu müssen~\cite{StudyRaid2024Composition}
    \item \textbf{Wiederverwendbarkeit:} Für Komposition entworfene Komponenten sind oft einfacher wiederzuverwenden, da sie keine Annahmen über den Kontext treffen, in dem sie verwendet werden~\cite{StudyRaid2024Composition}
    \item \textbf{Vermeidung von Tight Coupling:} Vererbung kann zu enger Kopplung zwischen Komponenten führen, was die Codebasis fragil und schwer zu refaktorisieren macht~\cite{StudyRaid2024Composition}
\end{itemize}

React fördert Komposition gegenüber Vererbung, weil sie größere Flexibilität und Trennung von Belangen ermöglicht~\cite{Mishra2024CompositionReact}. Vererbung kann manchmal zu enger Kopplung zwischen Komponenten führen, was es schwieriger macht, Anwendungen zu modifizieren oder zu skalieren~\cite{Mishra2024CompositionReact}. Durch die Übernahme von Komposition anstelle von Vererbung können Entwickler die Wiederverwendbarkeit von Komponenten vereinfachen und ihren Code flexibler und wartbarer gestalten~\cite{Mishra2024CompositionReact}.

\subsection{Full Stack React Frameworks}

Während React als \ac{UI}-Bibliothek exzelliert, benötigen produktionsreife Anwendungen zusätzliche Funktionalitäten wie Routing, Server-Side Rendering und Daten-Fetching. Full-Stack React Frameworks wie Next.js adressieren diese Anforderungen durch Bereitstellung einer kompletten Lösung für moderne Webanwendungen.

\textbf{Next.js als React-Backend-Framework:} Next.js hat sich zu einem Kraftpaket für den Aufbau performanter und \ac{SEO}-freundlicher React-Anwendungen entwickelt~\cite{Srivastava2025NextJSRendering}. Der App Router ist ein dateibasierter Router, der Reacts neueste Features wie Server Components, Suspense und Server Functions nutzt~\cite{NextJS2024AppRouter}. Next.js verwendet ein dateisystembasiertes Routing, bei dem Ordner zur Definition von Routen verwendet werden~\cite{NextJS2024DefiningRoutes}. Jeder Ordner repräsentiert ein Routen-Segment, das einem URL-Segment zugeordnet wird~\cite{NextJS2024DefiningRoutes}.

\textbf{File-Based Routing:} Mit dem App Router ermutigt Next.js zu einem dateibasierten Routing-System, bei dem die Verzeichnisstruktur die URL-Struktur widerspiegelt~\cite{Phutson2024NextJSStructure}. In Next.js nutzt der App Router das dateibasierte Routing, was bedeutet, dass die Position der \texttt{page.tsx}- oder \texttt{route.ts}-Datei innerhalb des \texttt{app}-Verzeichnisses definiert, wie sie auf eine gegebene URL abgebildet wird~\cite{ProNextJS2024FileRouting}.

Next.js folgt weiterhin dem dateibasierten Routing, aber mit der Einführung des App Routers haben Dateien und Ordner nun strikt definierte Rollen~\cite{Ansari2024NextJSAppRouter}:
\begin{itemize}
    \item \textbf{Ordner:} Ordner definieren die Routen der Anwendung. Ein Routen-Segment ist ein Pfad vom Root-Ordner bis zu einem Blatt-Ordner, der eine \texttt{page.ts}-Datei enthält~\cite{Ansari2024NextJSAppRouter}
    \item \textbf{Dateien:} Dateien erstellen die \ac{UI} für ein Routen-Segment~\cite{Ansari2024NextJSAppRouter}
\end{itemize}

Eine spezielle \texttt{page.ts}-Datei wird verwendet, um Routen-Segmente öffentlich zugänglich zu machen~\cite{NextJS2024DefiningRoutes}. Dynamic Route Segments können durch Umschließen des Ordnernamens mit eckigen Klammern erstellt werden, wie \texttt{[productId]}~\cite{Ansari2024NextJSAppRouter}.

\textbf{Rendering-Strategien:} Next.js ist zu einem führenden Framework geworden, indem es verschiedene Rendering-Strategien anbietet: \ac{SSG}, \ac{SSR}, \ac{CSR}, \ac{ISR} und das experimentelle \ac{PPR}~\cite{Vercel2024RenderingStrategy}. Diese wurden entwickelt, um Performance, \ac{SEO} und User Experience in verschiedenen Situationen zu optimieren~\cite{Vercel2024RenderingStrategy}.

\textbf{\ac{SSG}:} Bei \ac{SSG} wird die initiale \ac{HTML} zur Build-Zeit generiert, was zu schnellen Ladezeiten führt~\cite{Srivastava2025NextJSRendering}. \ac{SSG} ist ideal für Inhalte, die sich nicht häufig ändern, wie Blog-Posts, Dokumentation oder Marketing-Seiten~\cite{Srivastava2025NextJSRendering}. Im App Router-Modell rendert \ac{SSG} automatisch jede Komponente statisch, die keine server-spezifischen Funktionen nutzt~\cite{Udeji2024NextJS15Rendering}.

\textbf{\ac{SSR}:} \ac{SSR} generiert \ac{HTML} auf dem Server für jede Anfrage~\cite{Srivastava2025NextJSRendering}. Diese Strategie ist optimal für echtzeitbezogene, nutzerspezifische Inhalte wie personalisierte Dashboards, Account-Profile oder News-Feeds~\cite{Udeji2024NextJS15Rendering}. \ac{SSR} garantiert, dass jeder Besucher bei jedem Laden der Seite die neuesten Daten sieht~\cite{Udeji2024NextJS15Rendering}.

\textbf{\ac{ISR}:} \ac{ISR} baut auf \ac{SSG} auf und fügt die Fähigkeit hinzu, Inhalte zu aktualisieren, ohne einen vollständigen Rebuild der Site zu erfordern~\cite{Srivastava2025NextJSRendering}. Bei \ac{ISR} wird weiterhin die initiale \ac{HTML} zur Build-Zeit generiert, aber Next.js wird auch mitgeteilt, wie oft nach Updates geprüft und die \ac{HTML}-Dateien revalidiert werden sollen~\cite{Srivastava2025NextJSRendering}. \ac{ISR} ist geeignet für Inhalte, die sich periodisch ändern, wie Blog-Posts oder Produkt-Listings~\cite{Udeji2024NextJS15Rendering}.

\textbf{\ac{PPR}:} \ac{PPR} ist eine der neuesten Ergänzungen zu Next.js und befindet sich derzeit im experimentellen Status~\cite{Udeji2024NextJS15Rendering}. \ac{PPR} ermöglicht es, dass eine Seite teilweise mit statischen und dynamischen Segmenten kombiniert pre-gerendert wird~\cite{Udeji2024NextJS15Rendering}. Dies ist besonders nützlich für Seiten mit Sektionen, die progressiv laden können, während kritischer Content sofort erscheint~\cite{Udeji2024NextJS15Rendering}.

Partial Prerendering kombiniert ultra-schnelle statische Edge-Delivery mit vollständig dynamischen Fähigkeiten und hat das Potenzial, das Standard-Rendering-Modell für Webanwendungen zu werden~\cite{Vercel2024PartialPrerendering}. \ac{PPR} bietet ein vereinheitlichtes Modell, das die Zuverlässigkeit und Geschwindigkeit von \ac{ISR} mit den dynamischen Fähigkeiten von \ac{SSR} verbindet~\cite{Vercel2024PartialPrerendering}. 

\ac{PPR} basiert auf React Suspense Boundaries: zur Build-Zeit wird der gesamte Content bis zur Suspense-Boundary zusammen mit den Fallbacks statisch generiert und suspendiert das Rendering zur Build-Zeit~\cite{Snayak2024DissectingPPR}. Zur Request-Zeit wird die pre-gerenderte statische Shell sofort an den Client geliefert, während Next.js das Rendering dort fortsetzt, wo es zur Build-Zeit suspendiert wurde~\cite{Snayak2024DissectingPPR}. Sobald die suspendierten Children auflösen, wird die \ac{UI} zum Client in derselben Response gestreamt~\cite{Snayak2024DissectingPPR}.

Bei der Entscheidung für eine Rendering-Strategie sollten folgende Faktoren berücksichtigt werden: Wie oft ändert sich dieser Content? \ac{SSG} ist gut für statische Inhalte, \ac{ISR} ist hervorragend für periodisch wechselnde Inhalte, und \ac{SSR} oder \ac{CSR} ist am besten für Echtzeit-Daten~\cite{Vercel2024RenderingStrategy}. Next.js ermöglicht Entwicklern, verschiedene Rendering-Methoden innerhalb einer einzelnen Anwendung zu nutzen, je nach Bedarf, auf Seiten-Basis~\cite{Vercel2024RenderingStrategy}.

\section{UI/UX Frameworks \& Design System}

Die systematische Entwicklung von Benutzeroberflächen erfordert strukturierte Ansätze zur Gewährleistung von Konsistenz, Wartbarkeit und Skalierbarkeit. Moderne Web-Anwendungen stehen vor der Herausforderung, Interfaces für eine Vielzahl von Geräten, Bildschirmgrößen und Nutzungskontexten bereitzustellen. Design Systems und \ac{UI}-Frameworks bieten methodische Lösungen für diese Komplexität, wobei sich in den letzten Jahren fundamentale Paradigmenwechsel vollzogen haben.

\subsection{Grundlagen modularer Design-Systeme}

Die theoretische Grundlage moderner Design-Systeme bildet Brad Frosts Atomic Design Methodology, die 2013 erstmals als Blogpost vorgestellt und 2016 in Buchform veröffentlicht wurde~\cite{Frost2016AtomicDesign}. Inspiriert von chemischen Konzepten, postuliert Atomic Design, dass komplexe \ac{UI} systematisch in kleinere, wiederverwendbare Komponenten zerlegt werden können.

\textbf{Hierarchische Komponentenstruktur:} Die Methodologie definiert fünf hierarchische Ebenen~\cite{Frost2016AtomicDesign}:

\begin{itemize}
    \item \textbf{Atoms:} Fundamentale \ac{HTML}-Elemente wie Buttons, Input-Felder oder Labels, die nicht weiter zerlegt werden können ohne ihre Funktionalität zu verlieren.
    \item \textbf{Molecules:} Gruppen von Atoms, die zu funktionalen Einheiten kombiniert werden. Beispiel: Label, Input und Button bilden ein Search-Form-Molecule.
    \item \textbf{Organisms:} Komplexere Komponenten aus Molecules und Atoms, die eigenständige Interface-Sektionen bilden wie Navigation, Header oder Formulare.
    \item \textbf{Templates:} Layout-Strukturen, die Organisms arrangieren und die Content-Struktur ohne spezifischen Inhalt definieren.
    \item \textbf{Pages:} Konkrete Template-Instanzen mit realem Content, die für Usability-Testing und Design-Validation dienen.
\end{itemize}

\textbf{Vorteile modularer Komponentenarchitektur:} Der atomare Ansatz bietet mehrere fundamentale Vorteile~\cite{Frost2016AtomicDesign}:

\begin{itemize}
    \item \textbf{Konsistenz:} Wiederverwendbare Komponenten garantieren visuell und funktional einheitliche Interfaces.
    \item \textbf{Skalierbarkeit:} Die modulare Struktur vereinfacht langfristige Wartung und ermöglicht reibungslose Erweiterungen.
    \item \textbf{Effizienz:} Designer und Entwickler können Komponenten schnell kombinieren statt jedes Element neu zu erstellen.
    \item \textbf{Wartbarkeit:} Änderungen an einem Atom propagieren automatisch durch alle abhängigen Molecules und Organisms.
    \item \textbf{Shared Vocabulary:} Die hierarchische Nomenklatur schafft eine gemeinsame Sprache zwischen Design und Development.
\end{itemize}

\textbf{Traversierung zwischen Abstraktion und Konkretion:} Ein zentraler Vorteil von Atomic Design liegt in der Fähigkeit, simultan zwischen abstrakten Elementen und konkreten Interfaces zu wechseln~\cite{Frost2016AtomicDesign}. Designer können sowohl isolierte Atoms betrachten als auch deren Komposition in finalen Pages analysieren, was iterative Verfeinerung auf allen Hierarchieebenen ermöglicht.

\subsection{Utility-First CSS: Paradigmenwechsel im Styling}

Traditionelle \ac{CSS}-Methodologien wie \ac{BEM} und \ac{OOCSS} organisierten Styles primär um semantische Komponenten-Klassen. Der Utility-First-Ansatz vollzieht einen fundamentalen Paradigmenwechsel, indem Styles als komposierbare Utility-Klassen bereitgestellt werden, die direkte \ac{CSS}-Properties repräsentieren~\cite{TailwindDocs2024UtilityFirst}.

\textbf{Tailwind \ac{CSS} als Referenzimplementierung:} Tailwind \ac{CSS} etablierte sich als führendes Utility-First Framework~\cite{WikipediaTailwindCSS}. Im Gegensatz zu traditionellen Frameworks wie Bootstrap oder Foundation liefert Tailwind keine vorgefertigten Komponenten, sondern ein umfassendes Set von Utility-Klassen für Layout, Spacing, Typography, Colors und weitere \ac{CSS}-Properties~\cite{TailwindDocs2024UtilityFirst}.

Die Philosophie manifestiert sich in einem einfachen Beispiel:
\begin{lstlisting}
<!-- Traditionelles \ac{CSS} mit semantischen Klassen -->
<div class="message-warning">
  <p class="message-text">Warnung!</p>
</div>

<!-- Utility-First mit Tailwind -->
<div class="bg-yellow-300 font-bold p-4 rounded">
  <p class="text-gray-900">Warnung!</p>
</div>
\end{lstlisting}

\textbf{Constraints-basiertes Design:} Ein fundamentaler Vorteil von Utility-First liegt im constraints-basierten Design-Ansatz~\cite{TailwindDocs2024UtilityFirst}. Statt arbitrary "`Magic Numbers"' in Custom \ac{CSS} zu verwenden, wählen Entwickler aus einem vordefinierten Design System mit konsistenten Spacing-Skalen, Farbpaletten und Typographie-Hierarchien. Dies fördert visuell konsistente Interfaces und erleichtert Design Token Management.

\textbf{Technische Architektur und Evolution:} Tailwind \ac{CSS} durchlief signifikante architektonische Entwicklungen~\cite{WikipediaTailwindCSS}:

\begin{itemize}
    \item \textbf{Version 1-2:} Vollständige Stylesheet-Generierung mit nachgelagertem PurgeCSS Tree-Shaking. Nachteil: Lange Build-Zeiten und massive \ac{CSS}-Dateien vor Purging.
    \item \textbf{Version 3+ (\ac{JIT}-Mode):} \ac{JIT} Compiler analysiert \ac{HTML}/JSX/Vue-Files und generiert nur tatsächlich verwendete Utility-Klassen. Resultat: Drastisch reduzierte Build-Zeiten und Bundle-Größen.
    \item \textbf{Version 4 (aktuell):} Native \ac{CSS} Layer System (\texttt{@layer base, components, utilities}) eliminiert Specificity-Konflikte und ermöglicht klare Style-Hierarchien.
\end{itemize}

\textbf{Performance-Charakteristika:} Production Builds mit Tailwind \ac{CSS} generieren typischerweise \ac{CSS}-Bundles unter 10KB durch aggressive Unused-\ac{CSS}-Elimination~\cite{TailwindDocs2024UtilityFirst}. Diese Bundle-Größe ist signifikant kleiner als vollständige \ac{CSS}-Frameworks wie Bootstrap, was zu verbesserten \ac{FCP} Metriken führt.

\textbf{State Management und Responsiveness:} Tailwind bietet integrierte Variant-Systeme für States und Responsive Design~\cite{TailwindDocs2024UtilityFirst}:
\begin{lstlisting}
<button class="bg-blue-500 hover:bg-blue-700 
               md:w-auto sm:w-full 
               dark:bg-blue-900">
  Responsive Button
</button>
\end{lstlisting}

Diese Variants ermöglichen State-Handling (hover, focus, active) und Media Queries direkt im Markup, ohne separate \ac{CSS}-Dateien.

\textbf{Kritische Betrachtung:} Trotz der Vorteile existieren valide Kritikpunkte am Utility-First-Ansatz: Die Utility-First-Denkweise weicht fundamental von traditionellem \ac{CSS} ab und erfordert Einarbeitungszeit. \ac{HTML}-Elemente können mit vielen Utility-Klassen überladen werden, was die Lesbarkeit beeinträchtigt. Die Developer Experience wird jedoch durch moderne Tooling wie VSCode-Extensions mit vollständiger IntelliSense signifikant verbessert.

\subsection{Evolution der Komponenten-Bibliotheken}

Die Landschaft der UI-Komponenten-Bibliotheken durchlief mehrere Evolutionsstufen, die unterschiedliche Trade-offs zwischen Convenience und Control reflektieren.

\subsubsection{Generation 1: Opinionated Component Libraries}

Traditionelle \ac{UI}-Frameworks wie Bootstrap und \ac{MUI} liefern vollständige Design-Systeme mit vorgefertigten, gestylten Komponenten~\cite{MUIBlog2022IntroducingBase}. Diese Libraries folgen etablierten Design-Prinzipien -- Bootstrap mit eigener Design-Language, Material-UI mit Googles Material Design Guidelines~\cite{DevCommunity2025HeadlessVsTraditional}.

\textbf{Strukturelle Vorteile:}
\begin{itemize}
    \item \textbf{Rapid Prototyping:} Out-of-the-box Komponenten ermöglichen schnelle \ac{UI}-Entwicklung ohne Style-Implementation.
    \item \textbf{Design-Konsistenz:} Kohärente visuelle Sprache über alle Komponenten garantiert.
    \item \textbf{Große Communities:} Etablierte Frameworks bieten umfangreiche Dokumentation, Beispiele und Community-Support.
    \item \textbf{Accessibility Built-In:} Accessibility-Features wie \ac{ARIA}-Attributes und Keyboard-Navigation standardmäßig implementiert.
\end{itemize}

\textbf{Strukturelle Limitierungen:} Die fundamentalen Nachteile opinionated Libraries manifestieren sich in mehreren Dimensionen~\cite{MUIBlog2022IntroducingBase}:

\begin{itemize}
    \item \textbf{Design-Uniformität:} Viele Anwendungen teilen identische "`Bootstrap-Ästhetik"' oder "`Material Design Look"', was Brand-Differenzierung erschwert.
    \item \textbf{Override-Komplexität:} Abweichungen vom Default-Design erfordern tiefgreifende \ac{CSS}-Override-Kaskaden und Theme-System-Manipulation. Bei \ac{MUI} bedeutet dies häufig Kämpfe gegen Emotion's \ac{CSS}-in-JS Engine.
    \item \textbf{Bundle-Größe:} Comprehensive Libraries wie \ac{MUI} liefern signifikante JavaScript-Bundles inklusive Runtime-Styling-Engines, was \ac{FCP} und \ac{TTI} Metriken beeinträchtigt.
    \item \textbf{Vendor Lock-In:} Migration zu alternativen Design-Systemen erfordert Rewrite großer Codebases.
    \item \textbf{Specificity Wars:} Tief verschachtelte \ac{CSS}-Selektoren erschweren selektive Style-Overrides.
\end{itemize}

\subsubsection{Generation 2: Headless \ac{UI} Components}

Als Reaktion auf die Limitierungen opinionated Libraries entstand das Headless \ac{UI} Pattern: Komponenten liefern Funktionalität, Accessibility und State Management, aber keine visuellen Styles~\cite{DevCommunity2025HeadlessVsTraditional}.

\textbf{Separation of Concerns:} Headless Components implementieren strikte Trennung zwischen~\cite{LogRocket2024HeadlessShift}:
\begin{itemize}
    \item \textbf{Behavior Layer:} State Management, Event Handling, Keyboard Navigation
    \item \textbf{Accessibility Layer:} \ac{ARIA} Attributes, Screen Reader Support, Focus Management
    \item \textbf{Presentation Layer:} Vollständig Developer-kontrolliert, keine Default-Styles
\end{itemize}

\textbf{Prominente Implementierungen:} Radix \ac{UI} bietet unstyled Primitives für komplexe Patterns wie Dialogs, Dropdowns, Popovers und Tooltips mit strengem Fokus auf \ac{WAI}-\ac{ARIA} Compliance~\cite{WorkOS2025RadixVsShadcn}. Headless \ac{UI} von Tailwind Labs ist optimiert für Integration mit Tailwind \ac{CSS}. React Aria von Adobe fokussiert auf Internationalization und Platform-Agnostic Patterns~\cite{LogRocket2024HeadlessShift}.

\textbf{Vorteile des Headless-Patterns:}
\begin{itemize}
    \item \textbf{Design-Freiheit:} Keine Style-Overrides nötig, vollständige visuelle Kontrolle.
    \item \textbf{Framework-Agnostic:} Primitives integrieren mit beliebigen Styling-Solutions (Tailwind, Emotion, Vanilla \ac{CSS}).
    \item \textbf{Minimale Bundle-Größe:} Keine \ac{CSS}-Bundles oder Styling-Runtime.
    \item \textbf{Accessibility Garantiert:} Professionell implementierte \ac{ARIA}-Patterns ohne Developer-Overhead.
\end{itemize}

\textbf{Trade-offs:} Jede Component erfordert vollständiges Custom-Styling und kleinere Communities bedeuten weniger Community-Resources als etablierte \ac{UI}-Libraries~\cite{DevCommunity2025HeadlessVsTraditional}.

\ac{MUI} erkannte diese Entwicklung und führte Base \ac{UI} als eigene headless Component-Suite ein, die die Accessibility-Features von Material-\ac{UI} ohne visuelle Opinions bereitstellt~\cite{MUIBlog2022IntroducingBase}.

\subsubsection{Generation 3: Hybrid-Ansatz Shadcn/ui}

Shadcn/ui synthetisiert Headless Primitives mit production-ready Styling und etabliert ein innovatives Distribution-Modell~\cite{OpenReplay2024ShadcnReact}.

\textbf{Code Ownership Model:} Im Gegensatz zu traditionellen \ac{NPM}-Dependencies kopiert Shadcn/ui Component-Code direkt ins Projekt via \ac{CLI}~\cite{OpenReplay2024ShadcnReact}:
\begin{lstlisting}[language=bash]
npx shadcn-ui@latest init
npx shadcn-ui@latest add button dialog form
\end{lstlisting}

Resultat: Components existieren als modifizierbare Source-Files im Projekt, keine Black-Box-Dependency.

\textbf{Technische Foundation:}
\begin{itemize}
    \item \textbf{Radix \ac{UI} Primitives:} Accessibility und Behavior Layer~\cite{WorkOS2025RadixVsShadcn}
    \item \textbf{Tailwind \ac{CSS}:} Utility-First Styling
    \item \textbf{\ac{CVA}:} Type-safe Variant Management
    \item \textbf{TypeScript:} Vollständige Type-Safety
\end{itemize}

\textbf{Vorteile des Copy-Paste-Modells:}
\begin{itemize}
    \item \textbf{100\% Kontrolle:} Code im Projekt modifizierbar ohne Library-Constraints
    \item \textbf{Kein Vendor Lock-In:} Keine Runtime-Dependency auf Shadcn Package
    \item \textbf{Versionskontrolle:} Components unter eigener Git-History
    \item \textbf{Selective Adoption:} Nur benötigte Components im Projekt
\end{itemize}

\textbf{Trade-offs:} Updates müssen manuell integriert werden und das Modell erfordert Tailwind \ac{CSS} Expertise sowie aufwändigere initiale Konfiguration als \ac{NPM}-Install~\cite{OpenReplay2024ShadcnReact}.

\subsection{Graph-Visualisierung und automatische Layouts}

Die Visualisierung komplexer relationaler Datenstrukturen wie Quest-Dependencies oder Ressourcen-Hierarchien erfordert spezialisierte Bibliotheken für interaktives Graph-Rendering.

\textbf{React Flow als State-of-the-Art:} React Flow (seit 2024 rebrandend als XyFlow) etablierte sich als führende deklarative Bibliothek für node-basierte Editoren und interaktive Diagramme~\cite{ReactFlowDocs2024Layouting}. Die Library bietet:

\begin{itemize}
    \item \textbf{Deklarative API:} Nodes und Edges als React-Komponenten mit Props-basierter Konfiguration
    \item \textbf{Built-In Interaktivität:} Drag-and-Drop, Zoom, Pan ohne Custom-Implementation
    \item \textbf{Custom Node Types:} Vollständig anpassbare Node-Komponenten (z.B. Quest-Cards mit Thumbnails)
    \item \textbf{Performance-Optimierung:} Virtualisierung für Graphen mit tausenden Nodes
\end{itemize}

\textbf{Layout-Algorithmen:} Manuelle Node-Positionierung ist für komplexe Graphen impraktikabel. Automatische Layout-Algorithmen berechnen optimale Node-Platzierung basierend auf Graph-Struktur~\cite{ReactFlowDocs2024Layouting}.

\textit{Dagre -- Hierarchical Directed Graphs:} Der Dagre-Algorithmus optimiert für \ac{DAG} mit klarer Hierarchie~\cite{ReactFlowDocs2024Layouting}:

Dagre minimiert Edge-Crossings und optimiert Layer-Spacing. ~\cite{ReactFlowDocs2024Layouting}.

\textbf{Limitierungen:} Dagre setzt uniform Node-Dimensionen voraus. Custom Node-Sizes erfordern Post-Layout Adjustments. Dynamisches Re-Layout bei Graph-Änderungen muss manuell getriggert werden~\cite{ReactFlowDocs2024Layouting}.

\textit{Alternativen für komplexere Anforderungen:} ELK.js bietet extensive Konfiguration für diverse Graph-Typen (layered, force-directed, radial), wobei die Komplexität jedoch Support erschwert. D3-Force implementiert Physik-basierte Layouts für nicht-hierarchische Graphen mit Spring-Force-Simulation für organische Node-Distribution. D3-Hierarchy ist optimiert für Tree-Strukturen mit single Root Node~\cite{ReactFlowDocs2024Layouting}.

\textbf{Semantic-Specific Custom Algorithms:} Generische Layout-Algorithmen können semantische Bedeutung von Graphen nicht berücksichtigen~\cite{SplunkEngineering2022CustomGraphViz}. Für domain-spezifische Visualisierungen (z.B. Business Process Flows, Quest-Chains) können Custom-Algorithmen überlegen sein. Beispiel-Anforderungen: Sequentielle Quest-Chains sollten linear visualisiert werden, Parallel-Quests gruppieren sich visuell, Critical-Path-Highlighting erfordert spezifische Node-Platzierung. Die Entwicklung von Custom-Algorithmen folgt iterativen Refinement-Prozessen mit kontinuierlichem Visual-Feedback~\cite{SplunkEngineering2022CustomGraphViz}.

\section{State Management \& Data Fetching}

Die systematische Verwaltung von Anwendungszustand (State) stellt eine fundamentale Herausforderung in der Entwicklung moderner Webanwendungen dar. Die Evolution von State Management Lösungen im React-Ökosystem reflektiert eine kontinuierliche Auseinandersetzung mit den Trade-offs zwischen Komplexität, Developer Experience und Skalierbarkeit. Für die Arc Raiders Companion App ergibt sich die Notwendigkeit einer hybriden State Management Architektur, die sowohl lokalen \ac{UI}-State als auch Server-synchronisierte Daten effizient verwaltet.

\subsection{Evolution der State Management Paradigmen}

Die historische Entwicklung von State Management Libraries in React verdeutlicht einen Paradigmenwechsel von monolithischen, opinionated Lösungen hin zu spezialisierten, kompositionellen Architekturen.

\textbf{Die Flux-Architektur als konzeptuelle Grundlage:} Facebooks Einführung der Flux-Architektur um 2014 etablierte das Konzept unidirektionalen Datenflusses als Antwort auf die Probleme bidirektionaler Data Bindings~\cite{FrontendUndefined2024ReactStateHistory}. Die zentrale Innovation bestand in der Separation von State-Updates durch ein strukturiertes Muster: Actions beschreiben \textit{was} passiert ist, während Reducer definieren \textit{wie} der State sich ändert~\cite{Redux2024History}. Diese konzeptuelle Trennung adressierte die Unvorhersagbarkeit von Backbone-artigen Model-View Architekturen, bei denen Template-Updates zu unkontrollierbaren Kaskaden von State-Mutationen führen konnten~\cite{Redux2024History}.

\textbf{Redux als De-facto-Standard (2015--2019):} Dan Abramovs Redux, 2015 veröffentlicht, synthetisierte Flux-Prinzipien mit funktionaler Programmierung und wurde durch die Demonstration von Time-Travel Debugging bekannt~\cite{Redux2024History}. Bis 2016 etablierte sich die Konvention \glqq If you're using React, you must be using Redux too\grqq~\cite{Redux2024History}, was jedoch zu übermäßiger Adoption selbst in Kontexten führte, wo Redux nicht erforderlich war. Die zentrale Stärke von Redux manifestierte sich in der Vorhersagbarkeit: Ein zentraler Store, reine Reducer-Funktionen und immutable Updates ermöglichten deterministisches State Management~\cite{Redux2024History}.

\textbf{Das Boilerplate-Problem:} Die fundamentale Kritik an Redux fokussierte sich auf den exzessiven Boilerplate-Code~\cite{FrontendUndefined2024ReactStateHistory}. Die Implementation einer einzelnen State-Property erforderte die Erstellung von Action Types, Action Creators, Reducer Cases und Selectors. Für asynchrone Operationen wie \ac{API}-Requests mussten zusätzlich Middleware-Patterns (Redux Thunk, Redux Saga) mit separaten Actions für Loading-, Success- und Error-States implementiert werden~\cite{Redux2024History}. Diese Code-Proliferation führte zu signifikanter Entwicklungslatenz und erschwerte die Wartbarkeit, insbesondere in schnell iterierenden Projekten.

\textbf{Die Context \ac{API} Revolution (2019):} Die Einführung der modernisierten React Context \ac{API} mit korrekter Value-Propagation und die Verfügbarkeit von React Hooks veränderten die State Management Landschaft fundamental~\cite{Redux2024History}. Die ursprüngliche Context \ac{API} war \glqq essentially broken\grqq{} und konnte updated Values nicht korrekt propagieren, was Redux als faktisch einzige Option für Application-Wide State etablierte~\cite{Redux2024History}. Mit der neuen \ac{API} wurde \texttt{useState} mit \texttt{useContext} zu einer viablen Alternative für einfachere Anwendungsfälle, was die \glqq You don't need Redux at all\grqq{} Bewegung initiierte.

\textbf{Spezialisierung und Komposition (2019--heute):} Die Erkenntnis, dass nicht alle State-Kategorien identische Management-Patterns erfordern, führte zu spezialisierten Lösungen~\cite{Redux2024History}. Moderne Architekturen trennen explizit zwischen Client State Management (Zustand, Jotai, Recoil) und Server State Management (TanStack Query, \ac{SWR}, Apollo)~\cite{FrontendUndefined2024ReactStateHistory}. Diese Spezialisierung reduziert Komplexität durch Single Responsibility: Bibliotheken fokussieren sich auf spezifische Problemdomänen statt universeller Lösungen~\cite{Codemancers2024TanStackQuery}.

\subsection{Die Client-Server State Dichotomie}\label{sec:state-management-grundlagen}

Die fundamentale Unterscheidung zwischen Client State und Server State bildet die theoretische Grundlage moderner State Management Architekturen.

\textbf{Client State -- Kurzlebige \ac{UI}-Logik:} Client State umfasst Daten, die vollständig im Browser existieren und keine Server-Persistenz erfordern~\cite{DevCommunity2023ServerClientState}. Charakteristisch sind:

\begin{itemize}
    \item \textbf{Lokale \ac{UI}-Zustände:} Modal-Visibility, Dropdown-States, Form-Input-Werte, Tab-Selection
    \item \textbf{Temporäre Daten:} Multi-Step-Form Progress, Undo-Redo History, Draft-Content
    \item \textbf{View-Layer Optimierungen:} Scroll-Position, Pagination-Cursor, Filter-Kriterien
\end{itemize}

Die Management-Strategie für Client State priorisiert Entwicklerergonomie und React-Integration, da dieser State keine Synchronisation mit Backend-Systemen erfordert~\cite{DevCommunity2023ServerClientState}.

\textbf{Server State -- Asynchrone Remote-Daten:} Server State repräsentiert Daten, deren autoritative Quelle serverseitig liegt und die asynchron fetched, gecacht und synchronisiert werden müssen~\cite{TanStackQuery2024Overview}. Die inhärenten Charakteristika unterscheiden sich fundamental von Client State~\cite{DevCommunity2023ServerClientState}:

\begin{itemize}
    \item \textbf{Asynchronität:} Fetching erfolgt über Network Requests mit unvorhersagbarer Latenz
    \item \textbf{Shared Ownership:} Andere Clients können dieselben Daten parallel modifizieren
    \item \textbf{Staleness:} Gecachte Daten werden potenziell obsolet und erfordern Revalidierung
    \item \textbf{Persistenz:} Daten persistieren über Sessions und Devices hinweg
\end{itemize}

Die herkömmliche Verwaltung von Server State in globalen Client State Managern (Redux, MobX) führt zu suboptimalen Patterns~\cite{TanStackQuery2024Overview}. Entwickler müssen manuell Loading States, Error Handling, Cache Invalidation, Request Deduplication und Background Refetching implementieren -- Komplexität, die spezialisierte Libraries wie TanStack Query abstrahieren~\cite{Codemancers2024TanStackQuery}.

\subsection{Zustand als minimalistischer Client State Manager}

Zustand etablierte sich als moderne Alternative zu Redux durch radikale Vereinfachung bei Beibehaltung essentieller State Management Patterns~\cite{Refine2024Zustand}.

\textbf{Philosophische Grundprinzipien:} Zustand verfolgt eine \glqq barebones\grqq{} Philosophie: minimale \ac{API}-Surface, keine Opinions über State-Struktur, und vollständige Hook-basierte Integration~\cite{ZustandGitHub2024}. Im Gegensatz zu Redux erfordert Zustand keine Provider-Wrapping der Application, keine Action Types, keine Reducer Boilerplate~\cite{Refine2024Zustand}. Der gesamte Store wird durch eine einzelne \texttt{create}-Function definiert~\cite{Refine2024Zustand}:

\begin{lstlisting}
import { create } from 'zustand'

const useQuestStore = create((set, get) => ({
  // State
  activeFilter: 'all',
  selectedQuest: null,
  
  // Actions
  setFilter: (filter) => set({ activeFilter: filter }),
  selectQuest: (quest) => set({ selectedQuest: quest }),
  
  // Computed/Derived State via get()
  getFilteredQuests: () => {
    const filter = get().activeFilter
    // Filtering logic...
  }
}))
\end{lstlisting}

\textbf{Selektive Reactivity und Performance:} Zustand implementiert feingradiges Dependency Tracking durch Proxy-basierte Subscriptions~\cite{Refine2024Zustand}. Components subscriben nur zu spezifischen State-Slices via Selector-Functions, was unnötige Re-Renders minimiert~\cite{ZustandGitHub2024}:

\begin{lstlisting}
// Component re-rendert nur bei activeFilter Changes
const filter = useQuestStore(state => state.activeFilter)

// Component re-rendert nur bei selectedQuest Changes
const quest = useQuestStore(state => state.selectedQuest)
\end{lstlisting}

Diese automatische Optimization eliminiert die Notwendigkeit manueller Memoization via \texttt{React.memo} oder \texttt{useMemo}~\cite{Refine2024Zustand}.

\textbf{DevTools und Persistence:} Zustand bietet Middleware für Redux DevTools Integration und LocalStorage Persistence~\cite{ZustandGitHub2024}:

\begin{lstlisting}
import { devtools, persist } from 'zustand/middleware'

const useStore = create(
  devtools(
    persist(
      (set) => ({ /* store definition */ }),
      { name: 'quest-filter-storage' }
    )
  )
)
\end{lstlisting}

Die Persist-Middleware ermöglicht automatisches Speichern und Laden von State aus LocalStorage, essentiell für Features wie Quest-Filter-Präferenzen, die über Sessions persistieren sollen~\cite{Refine2024Zustand}.

\textbf{Trade-offs und Limitierungen:} Zustand's Minimalismus ist gleichzeitig Stärke und Limitation. Die Library bietet keine eingebauten Patterns für asynchrone Operations, keine strukturierten Side-Effect-Modelle wie Redux Saga, und ein kleineres Ecosystem als Redux~\cite{Refine2024Zustand}. Für komplexe State Machines oder stark strukturierte Business Logic kann Redux mit Redux Toolkit weiterhin überlegen sein.

\subsection{TanStack Query als Server State Spezialist}

TanStack Query (vormals React Query) revolutionierte Server State Management durch Abstraktion der inhärenten Komplexität asynchroner Datenoperationen~\cite{TanStackQuery2024Overview}.

\textbf{Deklaratives Data Fetching:} TanStack Query verschiebt Data Fetching von imperativem \texttt{useEffect}-Code zu deklarativen Query-Definitionen~\cite{Codemancers2024TanStackQuery}:

\begin{lstlisting}
import { useQuery } from '@tanstack/react-query'

function QuestList() {
  // Deklarative Query-Definition
  const { data, isLoading, error } = useQuery({
    queryKey: ['quests'],
    queryFn: () => fetch('/api/quests').then(r => r.json())
  })
  
  if (isLoading) return <LoadingSpinner />
  if (error) return <ErrorMessage error={error} />
  
  return <div>{data.map(quest => <QuestCard quest={quest} />)}</div>
}
\end{lstlisting}

Diese Deklaration eliminiert manuelle State-Variablen für \texttt{loading}, \texttt{error} und \texttt{data}, die traditionell in Redux oder lokalem State verwaltet werden mussten~\cite{TanStackQuery2024Overview}.

\textbf{Intelligent Caching und Query Keys:} Das Cache-System basiert auf Query Keys als eindeutigen Identifiern~\cite{Atlantbh2024TanStackQuery}. Beim Query-Execution prüft TanStack Query zunächst den Cache:

\begin{enumerate}
    \item Existiert gecachte Data für den Query Key?
    \item Ist die gecachte Data \glqq fresh\grqq{} basierend auf \texttt{staleTime}?
    \item Falls fresh: Sofortige Rückgabe aus Cache
    \item Falls stale: Background Refetch während gecachte Data angezeigt wird~\cite{Codemancers2024TanStackQuery}
\end{enumerate}

Die Parameter \texttt{staleTime} und \texttt{gcTime} (vormals \texttt{cacheTime}) kontrollieren dieses Verhalten~\cite{Atlantbh2024TanStackQuery}:

\begin{itemize}
    \item \textbf{staleTime:} Duration, für die Data als fresh gilt und nicht refetched wird (Default: 0ms)
    \item \textbf{gcTime:} Duration, für die unused Cache-Entries in Memory verbleiben (Default: 5min)~\cite{Atlantbh2024TanStackQuery}
\end{itemize}

\textbf{Request Deduplication:} Wenn multiple Components simultan denselben Query requesten, konsolidiert TanStack Query diese zu einem einzelnen Network Request~\cite{Codemancers2024TanStackQuery}. Alle subscribenden Components erhalten dieselbe Response, was Redundanz eliminiert:

\begin{lstlisting}
// Component A
const { data } = useQuery({ queryKey: ['quests'], queryFn: fetchQuests })

// Component B (mountet simultan)
const { data } = useQuery({ queryKey: ['quests'], queryFn: fetchQuests })

// Resultat: Nur 1 HTTP Request, beide Components erhalten Response
\end{lstlisting}

\textbf{Automatic Background Refetching:} TanStack Query implementiert intelligente Refetch-Strategien~\cite{TanStackQuery2024Overview}:

\begin{itemize}
    \item \textbf{Window Focus Refetching:} Queries refetchen, wenn User zur Tab zurückkehrt
    \item \textbf{Network Reconnection:} Automatic Refetch nach Netzwerk-Wiederverbindung
    \item \textbf{Polling:} Konfigurierbare Interval-basierte Refetches
\end{itemize}

Diese Features garantieren Data Freshness ohne manuelle Orchestration~\cite{Codemancers2024TanStackQuery}.

\textbf{Mutations und Optimistic Updates:} Für Data-Modification bietet TanStack Query \texttt{useMutation} mit Optimistic Update Support~\cite{TanStackQueryDocs2024OptimisticUpdates}:

\begin{lstlisting}
const updateQuestMutation = useMutation({
  mutationFn: (updates) => fetch(`/api/quests/${updates.id}`, {
    method: 'PATCH',
    body: JSON.stringify(updates)
  }),
  
  // Optimistic Update
  onMutate: async (updates) => {
    // Cancel outgoing refetches
    await queryClient.cancelQueries({ queryKey: ['quests', updates.id] })
    
    // Snapshot previous value
    const previous = queryClient.getQueryData(['quests', updates.id])
    
    // Optimistically update cache
    queryClient.setQueryData(['quests', updates.id], updates)
    
    // Return context for rollback
    return { previous }
  },
  
  // Rollback on error
  onError: (err, updates, context) => {
    queryClient.setQueryData(['quests', updates.id], context.previous)
  },
  
  // Refetch after success/error
  onSettled: (data, error, updates) => {
    queryClient.invalidateQueries({ queryKey: ['quests', updates.id] })
  }
})
\end{lstlisting}

Optimistic Updates ermöglichen instant \ac{UI}-Feedback: Der Cache wird sofort mit der antizipierten Response aktualisiert, bevor die Server-Response eintrifft~\cite{TanStackQueryDocs2024OptimisticUpdates}. Bei Fehlern erfolgt automatisches Rollback zur vorherigen Cache-Version.

\textbf{Cache Invalidation Strategien:} TanStack Query bietet granulare Cache Invalidation~\cite{TanStackQuery2024Overview}:

\begin{lstlisting}
// Invalidate specific query
queryClient.invalidateQueries({ queryKey: ['quests', questId] })

// Invalidate all quest queries
queryClient.invalidateQueries({ queryKey: ['quests'] })

// Invalidate with predicate
queryClient.invalidateQueries({
  predicate: (query) => query.queryKey[0] === 'quests' &&
   query.state.data?.status === 'active'
})
\end{lstlisting}

\textbf{Integration mit Server-Side Rendering:} TanStack Query unterstützt \ac{SSR} durch Dehydration/Hydration Patterns~\cite{TanStackQuery2024Overview}. Queries können serverseitig prefetched, dehydriert und im Client rehydriert werden, was \ac{FCP} optimiert:

\begin{lstlisting}
// Next.js getServerSideProps
export async function getServerSideProps() {
  const queryClient = new QueryClient()
  
  await queryClient.prefetchQuery({
    queryKey: ['quests'],
    queryFn: fetchQuests
  })
  
  return {
    props: {
      dehydratedState: dehydrate(queryClient)
    }
  }
}
\end{lstlisting}

\section{Datenbank und Backend Design}

Die Entwicklung moderner Webanwendungen erfordert fundierte Entscheidungen bezüglich der Backend-Architektur und Datenbankanbindung. Für die Arc Raiders Companion App stellt sich die Frage, welche Infrastruktur den optimalen Trade-off zwischen Entwicklungsgeschwindigkeit, Skalierbarkeit und langfristiger Wartbarkeit bietet. Die folgenden Abschnitte analysieren den Stand der Technik im Bereich Backend-as-a-Service-Plattformen und Object-Relational Mapping Lösungen.

\subsection{Backend-as-a-Service: Paradigmenwechsel in der Backend-Entwicklung}

\ac{BaaS} repräsentiert ein Cloud-Service-Modell, bei dem Entwickler alle serverseitigen Aspekte einer Web- oder Mobile-Anwendung auslagern können, um sich ausschließlich auf die Frontend-Entwicklung zu konzentrieren~\cite{Cloudflare2024BaaS}. \ac{BaaS}-Anbieter stellen vorgefertigte Software für serverseitige Aktivitäten bereit, darunter Benutzerauthentifizierung, Datenbankverwaltung, Remote-Updates und Push-Benachrichtigungen sowie Cloud-Speicher und Hosting~\cite{Cloudflare2024BaaS}.

\textbf{Konzeptuelle Grundlagen:} Das \ac{BaaS}-Paradigma abstrahiert die Komplexität der Backend-Infrastrukturverwaltung, einschließlich Serverwartung, Datenbankadministration und Skalierungsherausforderungen. Die modulare Architektur kombiniert einfach zu verwaltende Funktionalitäten, um komplexe Backend-Operationen für Entwickler zu vereinfachen. Typische \ac{BaaS}-Plattformen bieten \ac{REST} \ac{API}s, die die Verwaltung und Konfiguration von Architekturmodulen vereinfachen, einschließlich Authentifizierung und Datenbankänderungen.

\textbf{Strukturelle Vorteile und Limitierungen:} Mit vorgefertigten Backend-Services können Anwendungen schnell prototypisiert, entwickelt und deployed werden. Bei plötzlichem Nutzerwachstum skaliert die Infrastruktur im Hintergrund, was ein reibungsloses Benutzererlebnis gewährleistet. \ac{BaaS} eliminiert die Notwendigkeit großer Anfangsinvestitionen in Backend-Infrastruktur und ermöglicht Pay-as-you-use-Modelle~\cite{Cloudflare2024BaaS}. Als strukturelle Limitierung gilt jedoch das Risiko des Vendor Lock-In: Wenn eine \ac{BaaS}-Plattform genutzt wird, ist das Backend der Anwendung tief mit deren Infrastruktur und Services integriert, was einen späteren Anbieterwechsel erschwert. Während \ac{BaaS} Komfort und Effizienz bietet, kann das Niveau an Customization und Kontrolle geringer sein als bei maßgeschneiderter Entwicklung.

\textbf{Abgrenzung zu Serverless und \ac{FaaS}:} Es existiert eine gewisse Überschneidung zwischen \ac{BaaS} und Serverless Computing, da bei beiden der Entwickler nur den Anwendungscode schreiben muss~\cite{Cloudflare2024BaaS}. Die Backends von Serverless-Anwendungen sind jedoch in Funktionen aufgeteilt, die jeweils auf spezifische Events reagieren und nur eine Aktion ausführen. \ac{BaaS}-serverseitige Funktionalitäten sind dagegen nach Belieben des Anbieters konstruiert, und Entwickler müssen sich nur um das Frontend kümmern~\cite{Cloudflare2024BaaS}. \ac{BaaS} adressiert Backend-Funktionalität als Ganzes, während \ac{FaaS} nur Microservices in Anwendungen bedient, die auf auftretende Events reagieren.

\subsection{Supabase als Open-Source Backend-Plattform}

Supabase positioniert sich als Open-Source-Alternative zu Firebase und bietet eine Suite von Tools für sichere, skalierbare Web- und Mobile-Anwendungen~\cite{SupabaseGitHub2024}. Im Gegensatz zu Firebase, das primär NoSQL (Firestore) verwendet, basiert Supabase auf PostgreSQL und bringt ein standardbasiertes, relationales Datenmodell mit, während es die intuitive Entwicklererfahrung beibehält, die Firebase-Nutzer gewohnt sind.

\textbf{Architektonische Grundlagen:} Supabase kombiniert bewährte Open-Source-Tools wie PostgreSQL, Realtime, PostgREST, GoTrue, pg\_graphql und Kong zu einer modernen Entwicklungsplattform~\cite{SupabaseGitHub2024}. Die Philosophie lautet: Wenn Werkzeuge und Communities mit MIT-, Apache-2- oder äquivalenter Open-License existieren, werden diese genutzt und unterstützt. Wenn ein Werkzeug nicht existiert, wird es selbst entwickelt und als Open Source veröffentlicht~\cite{SupabaseGitHub2024}.

\textbf{PostgreSQL als Fundament:} PostgreSQL ist ein objektrelationales Datenbanksystem mit über 30 Jahren aktiver Entwicklung, das sich einen starken Ruf für Zuverlässigkeit, Feature-Robustheit und Performance erarbeitet hat~\cite{SupabaseGitHub2024}. Die Nutzung von PostgreSQL ermöglicht erweiterte SQL-Abfragen, \ac{RLS}, Transaktionen und strukturierte Datenschemata mit relationalen Constraints. Für viele Enterprise-Entwickler und erfahrene Ingenieure ist SQL essentiell, weshalb Supabase den optimalen Punkt zwischen einfachem Start und der Möglichkeit zur Skalierung trifft.

\subsubsection{Row Level Security: Datenbankseitige Zugriffskontrolle}

\ac{RLS} ist ein PostgreSQL-Sicherheitsfeature, das Datenbankadministratoren ermöglicht, Policies zu definieren, die kontrollieren, welche Zeilen Benutzer sehen oder modifizieren können~\cite{PostgreSQLDocsRLS2024}. RLS ist im Wesentlichen ein zusätzlicher Filter, der auf eine PostgreSQL-Datenbanktabelle angewendet wird. Wenn ein Benutzer versucht, eine Aktion auf einer Tabelle auszuführen, wird dieser Filter vor den Abfragekriterien angewendet, und die Daten werden gemäß der Sicherheitsrichtlinie eingeschränkt oder abgelehnt.

\textbf{Funktionsprinzip:} In PostgreSQL arbeiten RLS-Policies mit Rollen und Bedingungen~\cite{PostgreSQLDocsRLS2024}. Der allgemeine Prozess umfasst: Aktivierung von RLS auf Tabellenebene mit \texttt{ALTER TABLE ... ENABLE ROW LEVEL SECURITY}, Definition von Policies für spezifische Befehle (SELECT, INSERT, UPDATE, DELETE), Verwendung von Boolean-Ausdrücken zur Bestimmung sichtbarer/modifizierbarer Zeilen. Mehrere Policies können kombiniert werden: permissive Policies mit OR, restriktive Policies mit AND. Standardmäßig unterliegen Tabelleneigentümer nicht der Row-Level-Security, es sei denn, die Tabelle wird mit der Option \texttt{FORCE ROW LEVEL SECURITY} modifiziert~\cite{PostgreSQLDocsRLS2024}.

\textbf{Relevanz für Multi-Tenancy:} Für SaaS-Plattformen ist RLS als Grundlage für Multi-Tenancy absolut erwägenswert. Anstatt sich darauf zu verlassen, dass jeder Entwickler Tenant-Bedingungen in Code hinzufügt, garantiert RLS auf Datenbankebene, dass mandantenübergreifender Datenzugriff nicht passieren kann. Dies ermöglicht Defense in Depth: Selbst wenn der Anwendungscode einen Bug enthält, wird die Datenbank keine Daten außerhalb des Tenant-Bereichs zurückgeben oder modifizieren.

\textbf{Supabase-Integration:} Wenn granulare Autorisierungsregeln benötigt werden, ist PostgreSQL's Row Level Security unübertroffen~\cite{SupabaseDocsRLS2024}. Supabase ermöglicht bequemen und sicheren Datenzugriff vom Browser aus, solange RLS aktiviert ist. RLS muss immer auf Tabellen aktiviert sein, die in einem exponierten Schema gespeichert sind~\cite{SupabaseDocsRLS2024}. RLS ist unglaublich leistungsfähig und flexibel und ermöglicht das Schreiben komplexer SQL-Regeln, die einzigartigen Geschäftsanforderungen entsprechen. In Kombination mit Supabase Auth bietet es End-to-End-Benutzersicherheit vom Browser bis zur Datenbank~\cite{SupabaseDocsRLS2024}.

\textbf{Performance-Implikationen:} Jedes Autorisierungssystem hat Auswirkungen auf die Performance~\cite{SupabaseDocsRLS2024}. Dies gilt besonders für Abfragen, die jede Zeile einer Tabelle scannen, einschließlich vieler Select-Operationen mit Limit, Offset und Ordering. RLS-Policies werden für jede einzelne Zeile während der Abfrageausführung evaluiert. Best Practices umfassen: Hinzufügen von Indizes auf Spalten, die in Policies verwendet werden, Verwendung von \texttt{SECURITY DEFINER}-Funktionen zur Vermeidung verketteter RLS-Policies, und strategische Denormalisierung für konsistente RLS-Performance.

\subsubsection{Realtime: Echtzeit-Datensynchronisation}

Supabase Realtime ist ein Server, der mit Elixir unter Verwendung des Phoenix Frameworks entwickelt wurde und ermöglicht, auf Änderungen in der PostgreSQL-Datenbank via Logical Replication zu lauschen und diese dann über WebSockets zu broadcasten~\cite{SupabaseGitHubRealtime2024}.

\textbf{Architektur:} Supabase Realtime ist ein global verteilter Elixir-Cluster~\cite{SupabaseGitHubRealtime2024}. Clients können sich über WebSockets mit jedem Node im Cluster verbinden und Nachrichten an jeden anderen verbundenen Client senden. Phoenix ist schnell und kann Millionen gleichzeitiger Verbindungen handhaben, da Elixir leichtgewichtige Prozesse (keine OS-Prozesse) bereitstellt. Channels werden mit Phoenix Channels implementiert, die Phoenix.PubSub mit dem Standard-PG2-Adapter nutzen. Der PG2-Adapter verwendet Erlang-Prozessgruppen zur Implementierung des PubSub-Modells~\cite{SupabaseGitHubRealtime2024}.

\textbf{Funktionale Komponenten:} Das Realtime-System bietet drei Hauptfunktionen~\cite{SupabaseGitHubRealtime2024}:
\begin{itemize}
    \item \textbf{Broadcast:} Senden ephemerer Nachrichten von Client zu Clients mit niedriger Latenz
    \item \textbf{Presence:} Tracking und Synchronisation von geteiltem State zwischen Clients
    \item \textbf{Postgres Changes:} Lauschen auf PostgreSQL-Datenbankänderungen und Senden an autorisierte Clients
\end{itemize}

\textbf{Technische Implementation:} Bei der Kernfunktionalität pollt Supabase Realtime die integrierte Replikationsfunktionalität von PostgreSQL auf Datenbankänderungen, konvertiert Änderungen zu JSON und broadcastet das JSON über WebSockets an autorisierte Clients~\cite{SupabaseGitHub2024}. Diese Architektur ermöglicht Event-basiertes Lauschen auf INSERT, UPDATE, DELETE oder alle Events, Schema- und Tabellen-Targeting sowie granulares Filtering.

\subsubsection{Edge Functions: Serverless Computing am Netzwerkrand}

Supabase Edge Functions sind serverseitige TypeScript-Funktionen, die on-demand ausgeführt werden und global verteilt sind, was niedrige Latenzzeiten nahe der Nutzer gewährleistet~\cite{SupabaseDocsEdgeFunctions2024}. Sie werden mit Deno entwickelt, einem Open-Source-JavaScript-Runtime, das maximale Leistung und Flexibilität gewährleistet.

\textbf{Technische Grundlagen:} Edge Functions nutzen das Supabase Edge Runtime (ein Deno-kompatibles Runtime) mit TypeScript-First-Ansatz~\cite{SupabaseDocsEdgeFunctions2024}. Funktionen sind einfache .ts-Dateien, die einen Handler exportieren. Der Workflow umfasst: Bundling der Funktion mit Abhängigkeiten in ein ESZip-File, Upload zum Supabase-Backend, Generierung einer eindeutigen URL für globalen Zugriff. Selbst initiale Ausführungen sind schnell (Millisekunden) dank des kompakten ESZip-Formats und des minimalen Deno-Runtime-Overheads. Isolates können für einen Zeitraum aktiv bleiben, um nachfolgende Requests ohne Neustart zu handhaben, und mehrere Isolates können gleichzeitig am selben Edge-Standort laufen~\cite{SupabaseDocsEdgeFunctions2024}.

\textbf{Vorteile des Edge-Computing-Modells:} Die Nähe zu Nutzern reduziert Round-Trip-Zeiten erheblich. Das System handhabt variable Lasten ohne Server-Provisionierung automatisch. Native TypeScript-Unterstützung erhöht die Entwicklererfahrung, und die Open-Source-Natur ermöglicht Ausführung auf jeder Deno-kompatiblen Plattform~\cite{SupabaseDocsEdgeFunctions2024}.

\subsection{Object-Relational Mapping: Brücke zwischen Objekten und Relationen}

\ac{ORM} ist eine Programmiertechnik zur Konvertierung von Daten zwischen einer relationalen Datenbank und dem Speicher einer objektorientierten Programmiersprache~\cite{WikipediaORM2024}. Dies erzeugt effektiv eine virtuelle Objektdatenbank, die innerhalb des Programms verwendet werden kann.

\textbf{Das Object-Relational Impedance Mismatch:} Eine Vielzahl von Schwierigkeiten entsteht bei der Überlegung, wie ein Objektsystem mit einer relationalen Datenbank abgeglichen werden kann~\cite{WikipediaORM2024}. Diese Schwierigkeiten werden als Object-Relational Impedance Mismatch bezeichnet. Während Objekte Daten und Methoden enthalten, können relationale Datenbanken nur Daten speichern. Objekte sind oft aus anderen Objekten zusammengesetzt und Klassen erben Strukturen von anderen Klassen, aber relationalen Modellen fehlt ein eingebauter hierarchischer Mechanismus. Sie flachen die natürliche hierarchische Struktur eines Objekts in separate Tabellen mit atomaren Werten und eindeutigen Zeilen ab.

\textbf{Mapping-Patterns:} ORMs verwenden primär zwei Patterns zur Abbildung von Anwendungsobjekten auf Datenbankstrukturen: Das \textit{Active Record Pattern} verknüpft eine Anwendung mit einem Datenbankschema, setzt Tabellen mit Klassen, Zeilen mit Objekten und Spalten mit Attributen gleich. Jede Klasse stellt die grundlegenden Methoden für CRUD-Operationen bereit. Das \textit{Data Mapper Pattern} versucht hingegen, die Geschäftslogik in den Objekten von der Datenbank zu entkoppeln, was den Wechsel von Datenbanken erleichtert und die Wiederverwendung derselben Programmierlogik ermöglicht.

\textbf{Vorteile von \ac{ORM}-Werkzeugen:} Verglichen mit traditionellen Techniken des Austauschs zwischen objektorientierter Sprache und relationaler Datenbank reduziert ORM oft die Menge an Code, die geschrieben werden muss~\cite{WikipediaORM2024}. ORM verbirgt und kapselt Änderungen in der Datenquelle, sodass wenn sich Datenquellen oder ihre APIs ändern, nur ORM geändert werden muss. ORM-Tools sind darauf ausgelegt, die Möglichkeit von SQL-Injection-Angriffen zu eliminieren, da Queries prepared und sanitized werden.

\textbf{Nachteile von \ac{ORM}-Werkzeugen:} Nachteile von ORM-Tools resultieren generell aus dem hohen Abstraktionsgrad, der verbirgt, was tatsächlich im Implementierungscode passiert~\cite{WikipediaORM2024}. Aufgrund der Abstraktionsschicht kann Software mit ORM langsamer laufen als mit rohem SQL, besonders bei komplexen Queries gegen große Datensätze. Ein bekanntes Problem ist das N+1-Problem: Wenn über eine Collection iteriert und für jeden Eintrag dynamisch verwandte Zeilen gefetcht werden, queried das ORM jede einzelne Zeile separat.

\subsection{Drizzle ORM: TypeScript-native Datenbankabstraktion}

Drizzle ORM ist ein modernes TypeScript-First-Werkzeug, das die Arbeit mit Datenbanken einfacher und weniger fehleranfällig macht~\cite{DrizzleORMDocs2024}. Es positioniert sich komfortabel zwischen rohem SQL und TypeScripts Typsystem: Entwickler schreiben ihr Schema nach eigenem Stil, und Drizzle generiert automatisch Typen, sodass der Code sicher, sauber und konsistent bleibt.

\textbf{Philosophische Grundprinzipien:} Die Hauptphilosophie hinter Drizzle lautet: ``If you know SQL, you know Drizzle''~\cite{DrizzleORMDocs2024}. Im Gegensatz zur Abstraktion von SQL spiegelt Drizzle SQL in seiner API wider und bietet gleichzeitig einen höheren Abstraktionsgrad für Relational Queries. Drizzle nimmt einen anderen Ansatz als traditionelle ORMs: Es bietet einen typisierten SQL-Builder anstatt SQL hinter Abstraktionsschichten zu verstecken. Das Schema wird in TypeScript definiert, Queries geschrieben, die wie SQL aussehen (aber vollständig typsicher sind), und die Notwendigkeit für Runtime-Clients oder CLI-Generatoren entfällt.

\textbf{Architektonische Komponenten:} Statt eines monolithischen Designs folgt Drizzle einem dezentralisierten, composition-first Ansatz, bei dem jeder Teil eine klare, einzelne Verantwortung hat und unabhängig verwendet werden kann~\cite{DrizzleORMDocs2024}:

\begin{itemize}
    \item \textbf{Schema Definition:} Eine deklarative, TypeScript-basierte API zur Definition der Datenbankstruktur
    \item \textbf{SQL Driver Adapters:} Leichtgewichtige Wrapper um Datenbanktreiber, die standardisieren, wie Drizzle mit ihnen kommuniziert
    \item \textbf{Query Builder:} Ein Set komposierbarer Funktionen, die SQL generieren und dabei vollständige Type-Safety bewahren
    \item \textbf{Type Inference:} Ein leistungsstarkes System, das TypeScript-Typen direkt aus dem Schema extrahiert
    \item \textbf{Migration Tools:} Bereitgestellt durch drizzle-kit, helfen diese bei der Generierung und Ausführung von Datenbankmigrationen
\end{itemize}

\textbf{Type Safety und Developer Experience:} Eines der herausragenden Merkmale von Drizzle ORM ist Type Safety~\cite{LogRocketDrizzle2024}. TypeScripts statische Typprüfung stellt sicher, dass die Daten, die aus der Datenbank abgerufen werden, und die geschriebenen Queries konsistent mit dem definierten Datenbankschema sind. Dies hilft, Fehler früh im Entwicklungsprozess statt zur Laufzeit zu erkennen~\cite{LogRocketDrizzle2024}. Die Dinge, die Entwickler an Drizzle besonders schätzen, umfassen: Type Safety mit sehr starkem Typsystem, Performance-Optimierung, reichhaltige SQL-Dialekte, zero Dependencies, kein Code-Generation-Erfordernis, und Edge-Readiness.

\textbf{Code-First-Methodologie:} Drizzle ermutigt zu einer Code-First-Methodologie, bei der das TypeScript-Schema als Single Source of Truth dient. Das bedeutet, dass das Datenbankschema direkt im Codebase definiert wird, was Konsistenz und Versionskontrolle gewährleistet. Da Drizzle keinen Generate-Step wie Prisma nach jeder Schema-Änderung erfordert, funktionieren die Typen live und sind nicht von der Ausführung eines Befehls abhängig.

\section{Deployment und DevOps}

Die Bereitstellung moderner Webanwendungen erfordert ein fundiertes Verständnis der zugrundeliegenden Deployment-Strategien und DevOps-Praktiken. Die Arc Raiders Companion App steht vor der Herausforderung, einen effizienten, zuverlässigen und skalierbaren Deployment-Prozess zu etablieren. Die folgenden Abschnitte analysieren die theoretischen Grundlagen und den Stand der Technik im Bereich DevOps und modernes Web-Deployment.

\subsection{DevOps: Theoretische Fundierung und Kernprinzipien}

DevOps beschreibt die Integration und Automatisierung von Softwareentwicklung und IT-Operations mit dem Ziel, den Entwicklungszyklus zu verkürzen und eine kontinuierliche Auslieferung hochwertiger Software zu ermöglichen~\cite{KimHumbleDeboisWillis2016}. Obwohl keine einheitliche Definition existiert, wird DevOps allgemein als Bewegung verstanden, die darauf abzielt, die traditionelle Kluft zwischen Entwicklung (Dev) und Betrieb (Ops) zu überbrücken und schnellere, agilere Software-Deployments zu ermöglichen.

\textbf{Die Three Ways als theoretisches Fundament:} Kim et al. identifizieren in ``The DevOps Handbook'' drei fundamentale Prinzipien, aus denen sich alle DevOps-Patterns ableiten lassen~\cite{KimHumbleDeboisWillis2016}. Der \textit{First Way} (Flow) betont die Performance des gesamten Systems und fokussiert auf den schnellen Links-nach-Rechts-Fluss der Arbeit von Development über Operations zum Kunden. Dies beinhaltet, niemals bekannte Defekte an nachgelagerte Arbeitszentren weiterzugeben und stets ein tiefes Systemverständnis anzustreben. Der \textit{Second Way} (Feedback) etabliert Rechts-nach-Links-Feedback-Loops mit dem Ziel, diese zu verkürzen und zu verstärken, sodass notwendige Korrekturen kontinuierlich vorgenommen werden können. Der \textit{Third Way} (Continual Learning and Experimentation) kultiviert kontinuierliches Experimentieren, Risikobereitschaft und Lernen aus Fehlern sowie das Verständnis, dass Wiederholung und Übung die Voraussetzung für Meisterschaft sind~\cite{KimHumbleDeboisWillis2016}.

\textbf{Continuous Delivery als Enabler:} Humble und Farley definieren Continuous Delivery als die Fähigkeit, Änderungen aller Art -- einschließlich neuer Features, Konfigurationsänderungen, Bugfixes und Experimente -- sicher, schnell und nachhaltig in die Produktion zu bringen~\cite{HumbleFarley2010}. Die Autoren betonen, dass Wiederholbarkeit und Zuverlässigkeit aus zwei Prinzipien resultieren: der Automatisierung nahezu aller Prozesse und der Versionskontrolle aller Artefakte, die für Build, Deploy, Test und Release benötigt werden. Continuous Delivery unterscheidet sich von Continuous Deployment dadurch, dass bei ersterem die Automatisierung vor der Produktionsfreigabe pausiert und eine manuelle Freigabe erfordert, während letzteres den gesamten Release-Prozess vollständig automatisiert.

\subsection{DORA-Metriken: Empirische Messung von Software Delivery Performance}

Die wissenschaftliche Fundierung von DevOps-Praktiken wurde maßgeblich durch die Forschung von Forsgren, Humble und Kim vorangetrieben, deren vierjährige Studie in ``Accelerate'' die Grundlage für die heute weitverbreiteten DORA-Metriken bildet~\cite{ForsGrenHumbleKim2018}. Die Autoren entwickelten mittels rigoroser statistischer Methoden vier Schlüsselmetriken zur Messung der Software Delivery Performance.

\textbf{Throughput-Metriken:} Die \textit{Deployment Frequency} misst, wie häufig eine Organisation Code erfolgreich in Produktion deployt. Elite-Performer erreichen On-Demand-Deployments, oft mehrmals täglich, während Low-Performer monatlich oder seltener deployen~\cite{ForsGrenHumbleKim2018}. Die \textit{Lead Time for Changes} erfasst die durchschnittliche Zeit von einem Code-Commit bis zur produktiven Bereitstellung. Diese Metrik reflektiert die Effizienz des gesamten Software-Delivery-Prozesses und die Fähigkeit eines Teams, auf neue Anforderungen zu reagieren.

\textbf{Stability-Metriken:} Die \textit{Change Failure Rate} misst den Prozentsatz der Deployments, die zu Produktionsfehlern führen und Hotfixes oder Rollbacks erfordern. Eine niedrigere Rate indiziert einen zuverlässigeren Delivery-Prozess. Die \textit{Failed Deployment Recovery Time} (früher Mean Time to Recovery) erfasst die Zeit, die benötigt wird, um sich von einem fehlgeschlagenen Deployment zu erholen. Kürzere Recovery-Zeiten indizieren resilientere und reaktionsfähigere Systeme~\cite{DORAMetrics2024}.

\textbf{Zentrale Forschungsergebnisse:} Die DORA-Forschung belegt empirisch, dass Geschwindigkeit und Stabilität keine Trade-offs darstellen. Elite-Performer erreichen sowohl höhere Deployment-Frequenzen als auch niedrigere Fehlerraten~\cite{ForsGrenHumbleKim2018}. Organisationen, die Elite-Status in den DORA-Metriken erreichen, haben eine doppelt so hohe Wahrscheinlichkeit, ihre organisatorischen Leistungsziele zu erreichen. Die Studie identifiziert zudem 24 technische und kulturelle Capabilities, die höhere Performance treiben, darunter Versionskontrolle, automatisiertes Testing, Trunk-based Development und eine Kultur des kontinuierlichen Lernens.

\subsection{CI/CD-Pipelines: Automatisierung des Software-Lebenszyklus}

\ac{CI}/\ac{CD} bildet das technische Rückgrat moderner DevOps-Praktiken und automatisiert den gesamten Prozess von der Code-Integration bis zum Deployment. Continuous Integration bezeichnet die Praxis, neue Code-Änderungen automatisch zu bauen, zu testen und in ein Repository zu integrieren. Continuous Delivery bzw. Deployment automatisiert den nachfolgenden Bereitstellungsprozess~\cite{GitHubActions2024}.

\textbf{Strukturelle Komponenten:} Eine typische CI/CD-Pipeline umfasst mehrere Phasen: In der \textit{Build-Phase} werden Code und Abhängigkeiten zu einem ausführbaren Artefakt kompiliert. Die \textit{Test-Phase} validiert durch automatisierte Tests, dass der Code erwartungsgemäß funktioniert. In der \textit{Staging-Phase} wird die Anwendung in einer produktionsähnlichen Umgebung ausgeführt. Die \textit{Deployment-Phase} stellt die Anwendung automatisch den Endnutzern bereit~\cite{GitHubActions2024}.

\textbf{GitHub Actions als Implementierungsbeispiel:} GitHub Actions ist ein in GitHub integriertes Automatisierungswerkzeug, das Entwicklern ermöglicht, Workflows direkt in ihren Git-Repositories zu definieren. Workflows werden in YAML-Dateien spezifiziert und können durch verschiedene Events wie Push, Pull Request oder Schedule getriggert werden. Die Plattform unterstützt Linux, macOS, Windows und Container-Umgebungen und bietet Zugang zu einem umfangreichen Marketplace vorgefertigter Actions~\cite{GitHubActions2024}. Der State of DevOps Report zeigt, dass Organisationen mit ausgereiften CI/CD-Praktiken 208-mal häufiger deployen und eine 106-mal kürzere Lead Time aufweisen als der Rest~\cite{ForsGrenHumbleKim2018}.

\textbf{Vorteile automatisierter Pipelines:} CI/CD-Pipelines ermöglichen erhöhte Entwicklungsgeschwindigkeit durch schnelleres Feedback und häufigere, kleinere Commits. Die Stabilität und Zuverlässigkeit steigt, da automatisiertes Testing sicherstellt, dass Codebases jederzeit release-ready bleiben. Die Integration von Security-Checks (DevSecOps) ermöglicht frühzeitige Erkennung von Sicherheitsproblemen. Skalierbarkeit wird gewährleistet, da robuste CI/CD-Setups mühelos mit wachsenden Teams und Projektkomplexität expandieren können~\cite{GitHubActions2024}.

\subsection{Frontend-Deployment-Plattformen: Vercel als Framework-aware Infrastructure}

Moderne Frontend-Deployment-Plattformen wie Vercel repräsentieren einen Paradigmenwechsel in der Art, wie Webanwendungen bereitgestellt und betrieben werden. Vercel bietet eine auf Frontend-Entwicklung optimierte Cloud-Infrastruktur, die insbesondere für Next.js-Anwendungen entwickelt wurde~\cite{VercelDocs2024}.

\textbf{Framework-Defined Infrastructure:} Vercel implementiert das Konzept der Framework-Defined Infrastructure, bei dem die Infrastruktur automatisch basierend auf dem gewählten Framework konfiguriert wird. Bei einem Deploy analysiert Vercel die Anwendung, erkennt das Framework und richtet automatisch die optimalen Tools und Optimierungen ein~\cite{VercelDocs2024}. Für Next.js umfasst dies die automatische Konfiguration von Server-Side Rendering, Static Site Generation, Incremental Static Regeneration und Edge Functions.

\textbf{Serverless Functions und Edge Computing:} Vercel Functions ermöglichen die Ausführung serverseitigen Codes ohne Serververwaltung. Die Funktionen skalieren automatisch basierend auf der Nutzernachfrage. Im Gegensatz zu traditionellen Serverless Functions, die in spezifischen Regionen laufen, werden Edge Functions global an Edge-Standorten ausgeführt und reagieren signifikant schneller durch die Nähe zu den Nutzern~\cite{VercelDocs2024}. Edge Functions nutzen ein leichtgewichtiges Runtime basierend auf der V8-Engine und können bis zu 40\% schneller als herkömmliche Serverless Functions bei einem Bruchteil der Kosten sein.

\textbf{Fluid Compute:} Vercel's Fluid Compute-Technologie ermöglicht die gleichzeitige Ausführung mehrerer Requests innerhalb derselben Serverless-Instanz. Durch die Nutzung von Leerlaufzeiten für Compute werden Cold Starts reduziert, die Latenz gesenkt und Compute-Kosten gespart. Dies verhindert die Notwendigkeit, multiple isolierte Instanzen hochzufahren, wenn Tasks die meiste Zeit auf externe Operationen warten~\cite{VercelDocs2024}.

\subsection{Preview Deployments: Kollaborative Entwicklung durch Branch-basierte Umgebungen}

Preview Deployments repräsentieren einen fundamentalen Fortschritt in der kollaborativen Webentwicklung. Sie ermöglichen die automatische Erstellung einer Live-URL für jeden Branch oder Pull Request, sodass Änderungen in Isolation getestet und reviewed werden können, bevor sie in den Hauptbranch gemergt werden~\cite{VercelPreviewDocs2024}.

\textbf{Funktionsprinzip:} Standardmäßig erstellt Vercel ein Preview Deployment, wenn ein Commit zu einem Branch gepusht wird, der nicht der Production-Branch ist. Jedes Deployment erhält eine automatisch generierte URL, und Links erscheinen typischerweise in den PR-Kommentaren des Git-Providers. Dies ermöglicht instantanes visuelles Feedback für Reviewer, die Features ohne technisches Setup betrachten können, sichere Testumgebungen als einzigartige Sandboxes für jeden PR und schnellere, bessere Code-Reviews durch Einbeziehung von Design-Teams, QA und nicht-technischen Stakeholdern~\cite{VercelPreviewDocs2024}.

\textbf{Kollaborationsvorteile:} Preview Deployments reduzieren das ``It worked on my machine''-Problem und ermöglichen Teams, mit höherer Konfidenz zu shippen. Bugs oder Design-Probleme werden früher und im Kontext bemerkt, da jeder PR isoliert ist. Die Separation von Entwicklungs-, Staging- und Produktionsumgebungen verbessert die Zusammenarbeit und minimiert Risiken für Endnutzer während des gesamten Software Development Lifecycles~\cite{VercelPreviewDocs2024}.


\subsection{Theoretische Synthese für die Arc Raiders Companion App, (falsches kapitel?)}

Die Integration der beschriebenen Konzepte ermöglicht ein fundiertes theoretisches Framework für die Deployment-Strategie der Arc Raiders Companion App. Die Three Ways nach Kim et al. bilden das kulturelle und prozessuale Fundament, während die DORA-Metriken empirisch validierte Erfolgsindikatoren bereitstellen. Die Kombination von Vercel als Framework-aware Platform mit GitHub Actions für CI/CD implementiert die technischen Prinzipien von Continuous Delivery.

Für eine studentische Companion-App-Entwicklung bietet dieser Stack mehrere Vorteile: Die automatische Infrastrukturkonfiguration durch Framework-Defined Infrastructure eliminiert komplexe DevOps-Aufgaben. Preview Deployments ermöglichen kollaboratives Feedback ohne separate Staging-Infrastruktur. Die serverlose Architektur skaliert automatisch und minimiert Betriebskosten bei variablen Nutzungsmustern. Die Git-Integration gewährleistet Traceability und ermöglicht die Messung von Lead Time und Deployment Frequency als zentrale DORA-Metriken.

\section{Testing Frameworks \& Strategien}

  \subsection{Vitest}
  Unit und Integration Tests

  fast, modern, built for TS

  \subsection{Cypress}
  End-to-End Testing

  real browser testing
